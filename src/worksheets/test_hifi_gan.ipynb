{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c86ab599",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "nb_dir = Path.cwd()\n",
    "project_root = nb_dir.parent\n",
    "\n",
    "sys.path.append(str(project_root))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fa370d5c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'bigvgan'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[42]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      3\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchaudio\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mbigvgan\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mutils\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdata\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m create_dataloader\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'bigvgan'"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torchaudio\n",
    "import bigvgan\n",
    "\n",
    "from utils.data import create_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132bfb2a",
   "metadata": {},
   "source": [
    "### Sample Dataset Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6ce71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = create_dataloader(\n",
    "    root_dir=\"../../data/wav\",\n",
    "    batch_size=1,\n",
    "    segment_seconds=2.0,\n",
    "    sample_rate=22050,\n",
    "    n_fft=1024,\n",
    "    hop_length=256,\n",
    "    n_mels=80,\n",
    "    return_waveform=True,\n",
    "    num_workers=0,  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d802de0",
   "metadata": {},
   "source": [
    "### Loading Vocos model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30bfa39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "TorchCodec is required for load_with_torchcodec. Please install torchcodec to use this function.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Repos/ArcaneSequencer/.venv/lib/python3.14/site-packages/torchaudio/_torchcodec.py:82\u001b[39m, in \u001b[36mload_with_torchcodec\u001b[39m\u001b[34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size, backend)\u001b[39m\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchcodec\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdecoders\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AudioDecoder\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'torchcodec'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m device = torch.device(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mUsing device:\u001b[39m\u001b[33m\"\u001b[39m, device)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m batch = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mloader\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      5\u001b[39m mel = batch[\u001b[33m\"\u001b[39m\u001b[33mmel\u001b[39m\u001b[33m\"\u001b[39m]          \u001b[38;5;66;03m# [B, 1, n_mels, T]\u001b[39;00m\n\u001b[32m      6\u001b[39m wav = batch[\u001b[33m\"\u001b[39m\u001b[33mwaveform\u001b[39m\u001b[33m\"\u001b[39m]     \u001b[38;5;66;03m# [B, 1, segment_samples]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Repos/ArcaneSequencer/.venv/lib/python3.14/site-packages/torch/utils/data/dataloader.py:732\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    729\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    730\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    731\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m732\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    733\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    734\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    735\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    736\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    738\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Repos/ArcaneSequencer/.venv/lib/python3.14/site-packages/torch/utils/data/dataloader.py:788\u001b[39m, in \u001b[36m_SingleProcessDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    786\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    787\u001b[39m     index = \u001b[38;5;28mself\u001b[39m._next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m788\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[32m    789\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pin_memory:\n\u001b[32m    790\u001b[39m         data = _utils.pin_memory.pin_memory(data, \u001b[38;5;28mself\u001b[39m._pin_memory_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Repos/ArcaneSequencer/.venv/lib/python3.14/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[39m, in \u001b[36m_MapDatasetFetcher.fetch\u001b[39m\u001b[34m(self, possibly_batched_index)\u001b[39m\n\u001b[32m     50\u001b[39m         data = \u001b[38;5;28mself\u001b[39m.dataset.__getitems__(possibly_batched_index)\n\u001b[32m     51\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m         data = [\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[32m     53\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     54\u001b[39m     data = \u001b[38;5;28mself\u001b[39m.dataset[possibly_batched_index]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Repos/ArcaneSequencer/src/utils/data.py:120\u001b[39m, in \u001b[36mMusicChunkDataset.__getitem__\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, idx: \u001b[38;5;28mint\u001b[39m) -> Dict[\u001b[38;5;28mstr\u001b[39m, Any]:\n\u001b[32m    119\u001b[39m     path = \u001b[38;5;28mself\u001b[39m.files[idx % \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.files)]\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m     waveform = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_load_audio\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m          \u001b[38;5;66;03m# [1, N]\u001b[39;00m\n\u001b[32m    121\u001b[39m     chunk = \u001b[38;5;28mself\u001b[39m._random_chunk(waveform)       \u001b[38;5;66;03m# [1, segment_samples]\u001b[39;00m\n\u001b[32m    122\u001b[39m     mel = \u001b[38;5;28mself\u001b[39m._wav_to_logmel(chunk)           \u001b[38;5;66;03m# [1, n_mels, T]\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Repos/ArcaneSequencer/src/utils/data.py:82\u001b[39m, in \u001b[36mMusicChunkDataset._load_audio\u001b[39m\u001b[34m(self, path)\u001b[39m\n\u001b[32m     80\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_load_audio\u001b[39m(\u001b[38;5;28mself\u001b[39m, path: Path) -> torch.Tensor:\n\u001b[32m     81\u001b[39m     \u001b[38;5;66;03m# waveform: [channels, num_samples], sr: int\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m     waveform, sr = \u001b[43mtorchaudio\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     84\u001b[39m     \u001b[38;5;66;03m# To mono (average channels)\u001b[39;00m\n\u001b[32m     85\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.mono \u001b[38;5;129;01mand\u001b[39;00m waveform.shape[\u001b[32m0\u001b[39m] > \u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Repos/ArcaneSequencer/.venv/lib/python3.14/site-packages/torchaudio/__init__.py:86\u001b[39m, in \u001b[36mload\u001b[39m\u001b[34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size, backend)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(\n\u001b[32m     19\u001b[39m     uri: Union[BinaryIO, \u001b[38;5;28mstr\u001b[39m, os.PathLike],\n\u001b[32m     20\u001b[39m     frame_offset: \u001b[38;5;28mint\u001b[39m = \u001b[32m0\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m     26\u001b[39m     backend: Optional[\u001b[38;5;28mstr\u001b[39m] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     27\u001b[39m ) -> Tuple[torch.Tensor, \u001b[38;5;28mint\u001b[39m]:\n\u001b[32m     28\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Load audio data from source using TorchCodec's AudioDecoder.\u001b[39;00m\n\u001b[32m     29\u001b[39m \n\u001b[32m     30\u001b[39m \u001b[33;03m    .. note::\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     84\u001b[39m \u001b[33;03m        by TorchCodec.\u001b[39;00m\n\u001b[32m     85\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mload_with_torchcodec\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[43m        \u001b[49m\u001b[43muri\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[43m        \u001b[49m\u001b[43mframe_offset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mframe_offset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     89\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_frames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     90\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     91\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchannels_first\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchannels_first\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mformat\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     93\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     94\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbackend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     95\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Repos/ArcaneSequencer/.venv/lib/python3.14/site-packages/torchaudio/_torchcodec.py:84\u001b[39m, in \u001b[36mload_with_torchcodec\u001b[39m\u001b[34m(uri, frame_offset, num_frames, normalize, channels_first, format, buffer_size, backend)\u001b[39m\n\u001b[32m     82\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorchcodec\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdecoders\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AudioDecoder\n\u001b[32m     83\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[32m     85\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mTorchCodec is required for load_with_torchcodec. \u001b[39m\u001b[33m\"\u001b[39m \u001b[33m\"\u001b[39m\u001b[33mPlease install torchcodec to use this function.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     86\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m     88\u001b[39m \u001b[38;5;66;03m# Parameter validation and warnings\u001b[39;00m\n\u001b[32m     89\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m normalize:\n",
      "\u001b[31mImportError\u001b[39m: TorchCodec is required for load_with_torchcodec. Please install torchcodec to use this function."
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "batch = next(iter(loader))\n",
    "mel = batch[\"mel\"]          # [B, 1, n_mels, T]\n",
    "wav = batch[\"waveform\"]     # [B, 1, segment_samples]\n",
    "paths = batch[\"path\"]\n",
    "\n",
    "print(\"Loaded file:\", paths[0])\n",
    "print(\"mel shape:\", mel.shape)\n",
    "print(\"waveform shape:\", wav.shape)\n",
    "\n",
    "\n",
    "model = bigvgan.BigVGAN.from_pretrained(\n",
    "    \"nvidia/bigvgan_v2_22khz_80band_256x\",\n",
    "    use_cuda_kernel=False,\n",
    ").to(device)\n",
    "\n",
    "# remove weight norm and set eval mode as recommended\n",
    "model.remove_weight_norm()\n",
    "model.eval()\n",
    "\n",
    "# BigVGAN expects [B, C, T] with C = 80 mels\n",
    "mel_for_vocoder = mel.squeeze(1).to(device)  # [B, 80, T]\n",
    "\n",
    "with torch.inference_mode():\n",
    "    audio_gen = model(mel_for_vocoder)       # [B, 1, num_samples]\n",
    "\n",
    "print(\"Generated audio shape:\", audio_gen.shape)\n",
    "\n",
    "sr = model.h.sampling_rate  # BigVGAN’s native sampling rate (≈ 22050/22000)\n",
    "\n",
    "print(\"Vocoder sample rate:\", sr)\n",
    "\n",
    "out_dir = Path(\"outputs\")\n",
    "out_dir.mkdir(exist_ok=True)\n",
    "\n",
    "# 3) Save original chunk and vocoder output for comparison\n",
    "torchaudio.save(\n",
    "    out_dir / \"original_chunk.wav\",\n",
    "    wav[0].cpu(),\n",
    "    sample_rate=sr,  # or 22050, they should be very close\n",
    ")\n",
    "\n",
    "torchaudio.save(\n",
    "    out_dir / \"bigvgan_from_mel.wav\",\n",
    "    audio_gen[0].cpu(),\n",
    "    sample_rate=sr,\n",
    ")\n",
    "\n",
    "print(\"Saved:\")\n",
    "print(\" - outputs/original_chunk.wav\")\n",
    "print(\" - outputs/bigvgan_from_mel.wav\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "arcanesequencer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
